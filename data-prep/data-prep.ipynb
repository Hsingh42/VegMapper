{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stack Preparation (data-prep)\n",
    "\n",
    "This tutorial will walk you through the workflow of the [VegMapper](https://github.com/NaiaraSPinto/VegMapper) repo. At the end of this tutorial, you will create multi-band geotiffs that can be used for the identification and classification of specific agroforestry systems, such as palm-oil plantations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get credentials ##\n",
    "\n",
    "1) NASA Earthdata: https://urs.earthdata.nasa.gov/users/new\n",
    "\n",
    "2) JAXA: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/registration.htm\n",
    "\n",
    "3) AWS S3/EC2: https://portal.aws.amazon.com/billing/signup#/start\n",
    "\n",
    "4) Google Earth Engine: https://earthengine.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-time authentication for Google Earth Engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=tPRoOEER_31RyKhtFiGR--ZbNe7RXGpnzs11mPoA2pg&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=tPRoOEER_31RyKhtFiGR--ZbNe7RXGpnzs11mPoA2pg&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: \n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Problem requesting tokens. Please try again.  HTTP Error 400: Bad Request b'{\\n  \"error\": \"invalid_request\",\\n  \"error_description\": \"Missing required parameter: code\"\\n}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\site-packages\\ee\\oauth.py\u001b[0m in \u001b[0;36mrequest_token\u001b[1;34m(auth_code, code_verifier)\u001b[0m\n\u001b[0;32m     83\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     response = request.urlopen(\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mTOKEN_URI\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8716/2184186848.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mee\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAuthenticate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\site-packages\\ee\\__init__.py\u001b[0m in \u001b[0;36mAuthenticate\u001b[1;34m(authorization_code, quiet, code_verifier)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mcode_verifier\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPKCE\u001b[0m \u001b[0mverifier\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprevent\u001b[0m \u001b[0mauth\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mstealing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \"\"\"\n\u001b[1;32m---> 89\u001b[1;33m   \u001b[0moauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthorization_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode_verifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\site-packages\\ee\\oauth.py\u001b[0m in \u001b[0;36mauthenticate\u001b[1;34m(cli_authorization_code, quiet, cli_code_verifier)\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mwebbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m   \u001b[0m_obtain_and_write_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode_verifier\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Will prompt for auth_code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\site-packages\\ee\\oauth.py\u001b[0m in \u001b[0;36m_obtain_and_write_token\u001b[1;34m(auth_code, code_verifier)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mauth_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enter verification code: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth_code\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode_verifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m   \u001b[0mwrite_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nSuccessfully saved authorization token.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\data-prep\\lib\\site-packages\\ee\\oauth.py\u001b[0m in \u001b[0;36mrequest_token\u001b[1;34m(auth_code, code_verifier)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mrefresh_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'refresh_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     raise Exception('Problem requesting tokens. Please try again.  %s %s' %\n\u001b[0m\u001b[0;32m     90\u001b[0m                     (e, e.read()))\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Problem requesting tokens. Please try again.  HTTP Error 400: Bad Request b'{\\n  \"error\": \"invalid_request\",\\n  \"error_description\": \"Missing required parameter: code\"\\n}'"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Set up data-prep conda environment ##\n",
    "\n",
    "To create **data-prep** environment and install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate data-prep\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG menuinst_win32:__init__(200): Menu: name: 'Anaconda${PY_VER} ${PLATFORM}', prefix: 'D:\\anaconda3\\envs\\data-prep', env_name: 'data-prep', mode: 'user', used_mode: 'user'\n",
      "DEBUG menuinst_win32:create(324): Shortcut cmd is D:\\anaconda3\\python.exe, args are ['D:\\\\anaconda3\\\\cwp.py', 'D:\\\\anaconda3\\\\envs\\\\data-prep', 'D:\\\\anaconda3\\\\envs\\\\data-prep\\\\Library\\\\bin\\\\bash.exe', '--login', '-i', '--']\n"
     ]
    }
   ],
   "source": [
    "!conda env create -f data-prep-env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate **data-prep** environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate data-prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run **setup.py** to verify environment and permissions of scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALOS-2\\alos2_download_mosaic.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\remyw\\OneDrive - California Polytechnic State University\\HDD\\cal poly\\classes\\deep gis\\palm oil\\VegMapper\\data-prep\\setup.py\", line 22, in <module>\n",
      "    copyfile(src, dst)\n",
      "  File \"D:\\anaconda3\\envs\\data-prep\\lib\\shutil.py\", line 265, in copyfile\n",
      "    with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\anaconda3\\\\envs\\\\data-prep\\\\bin\\\\alos2_download_mosaic.py'\n"
     ]
    }
   ],
   "source": [
    "!python setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Prepare UTM tiles for AOI ##\n",
    "\n",
    "### prep_tiles.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % prep_tiles.py [-h] (--aoishp aoishp | --admshp admshp) aoi_name tile_size\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Prepare boundary and tiles for AOI.\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **aoi_name**\n",
    "\n",
    "    name of area of interest (AOI)\n",
    "\n",
    "  **tile_size**        \n",
    "\n",
    "    tile size in meters\n",
    "\n",
    "### Optional Arguments ###\n",
    "\n",
    "  **--aoishp**\n",
    "  \n",
    "    aoishp  shp/geojson of AOI\n",
    "\n",
    "  **--admshp** \n",
    "\n",
    "    admshp  shp/geojson of subnational administrative boundaries\n",
    "    \n",
    "Create tiles for the Ucayali region of Peru with a tile size of 100 meters:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: what's the difference between aoishp and admshp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Utils/prep_tiles.py --aoishp AOI/ucayali/ucayali_boundary.geojson ucayali_tiles_1.geojson 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Prepare Sentinel-1 Tiles ##\n",
    "\n",
    "## Search Sentinel-1 granules on [ASF Vertex](https://search.asf.alaska.edu/#/) ##\n",
    "\n",
    "1. Sign in using your Earthdata credentials. If you haven't used ASF Vertex before, you will need to agree their terms in order to use their HyP3 processing.\n",
    "\n",
    "2. Use following \"Additional Filters\" when searching granules for your AOI:\n",
    "\n",
    "    * File Type: L1 Detected High-Res Dual-Pol (GRD-HD)\n",
    "    * Beam Mode: IW\n",
    "    * Polarization: VV+VH\n",
    "\n",
    "    ![vertex_search_filters](img/vertex_search_filters.png)\n",
    "\n",
    "3. Add selected granules into download queue:\n",
    "\n",
    "    ![vertex_add_queue](img/vertex_add_queue.png)\n",
    "\n",
    "4. Download metadata files. At least download one csv or geojson file, which will be used for submitting HyP3 jobs.\n",
    "\n",
    "    ![vertex_download_metadata](img/vertex_download_metadata.png)\n",
    "\n",
    "5. Clear the selected granules in the downloads. Do not download these GRD-HD products as we will submit HyP3 jobs to apply radiometric terrain correction (RTC) to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit HyP3 RTC jobs ##\n",
    "\n",
    "### s1_submit_hyp3_jobs.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % s1_submit_hyp3_jobs.py [-h] [--dst dstpath] csv/geojson\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "    \n",
    "Submit ASF HyP3 RTC processing jobs for Sentinel-1 granules.\n",
    "\n",
    "Optionally copy or download the processed granules to the following destinations:\n",
    "\n",
    "* AWS S3 bucket - s3://bucket/prefix\n",
    "\n",
    "* Google Cloud Storage (GCS) - gs://bucket/prefix\n",
    "\n",
    "* Local storage\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **csv/geojson**\n",
    "  \n",
    "      Metadata file (csv or geojson) downloaded from ASF Vertex after data search.\n",
    "  \n",
    "### Optional Arguments ###\n",
    "\n",
    "  **--dst dstpath**\n",
    "  \n",
    "      Destination path to store the processed granules.\n",
    "      Supported dstpaths: AWS S3 - s3://your_bucket/some_prefix\n",
    "                       GCS - gs://your_bucket/some_prefix\n",
    "                       Local storage - your_local_path\n",
    "      If dstpath not provided, AWS S3 buckets/keys for the processed granules will be displayed at the end.\n",
    "      The processed granules will be saved in the following directory structure:\n",
    "          dstpath\n",
    "           └──year\n",
    "               └──path_frame\n",
    "                   └──processed_granules\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* Since ASF HyP3 stores the processed granules in their AWS S3 buckets, the data transfer will be much faster if you set up your S3 bucket to host these data. That is, using **s3://your_bucket/some_prefix** for --dst option.\n",
    "\n",
    "* To set up AWS Command Line Interface (CLI) configurations and credentials (required if your dstpath is S3 or GCS):\n",
    "\n",
    "    ```\n",
    "    (data-prep) % aws configure\n",
    "    ```\n",
    "\n",
    "    where you will be asked to enter your **aws_access_key_id** and **aws_secret_access_key**.\n",
    "\n",
    "* To set up Google Cloud gsutil tool (required if your dstpath is GCS):\n",
    "\n",
    "    ```\n",
    "    (data-prep) % gsutil config\n",
    "    ```\n",
    "\n",
    "    Then you will be prompted to sign in using your Google credentials. \n",
    "\n",
    "### Examples ###\n",
    "\n",
    "* Run without --dst option\n",
    "\n",
    "```\n",
    "(data-prep) % s1_submit_hyp3_jobs.py 2017_94_215.geojson\n",
    "Destination path for processed granules not provided. The download links will be listed at the end.\n",
    "\n",
    "Enter Earthdata Username: xxxxxx\n",
    "Enter Earthdata Password: \n",
    "\n",
    "Your remaining quota for HyP3 jobs: 977 granules.\n",
    "\n",
    "You will be submitting the following granules for HyP3 RTC processing:\n",
    "    2017_94_215 - 23 granules\n",
    "\n",
    "Enter 'Y' to confirm you would like to submit these granules, or 'N' if you have already submitted the granules and want to copy the processed granules to your dstpath: N\n",
    "\n",
    "Your processed granules for year_path_frame 2017_94_215 are available here:\n",
    "\n",
    "hyp3-contentbucket-abcdefghijkl/12345abc-wxyz-1234-5678-opqrstuvwxyz/S1B_IW_20170111T032030_DVP_RTC30_G_gpunem_C229.zip\n",
    "Expiration Time: 2021-09-18 00:00:00+00:00\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "Done with everything.\n",
    "```\n",
    "\n",
    "* Run with --dst option pointed to a S3 bucket\n",
    "\n",
    "```\n",
    "(data-prep) % s1_submit_hyp3_jobs.py 2017_94_215.geojson --dst s3://servir-public/test\n",
    "\n",
    "Enter Earthdata Username: xxxxxx\n",
    "Enter Earthdata Password: \n",
    "\n",
    "Your remaining quota for HyP3 jobs: 977 granules.\n",
    "\n",
    "You will be submitting the following granules for HyP3 RTC processing:\n",
    "    2017_94_215 - 23 granules\n",
    "\n",
    "Enter 'Y' to confirm you would like to submit these granules, or 'N' if you have already submitted the granules and want to copy the processed granules to your dstpath: N\n",
    "\n",
    "2017_94_215: copying processed granules to s3://servir-public/test\n",
    "2017_94_215: DONE copying processed granules to s3://servir-public/test\n",
    "\n",
    "Done with everything.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Sentinel/s1_submit_hyp3_jobs.py . Sentinel/granules/ucayali/ucayali_sentinel_granules_2017.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 Processing ##\n",
    "\n",
    "### s1_proc.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % s1_proc.py [-h] [--pf path_frame] [--m1 m1] [--m2 m2] srcpath year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "    \n",
    "Processing Sentinel-1 RTC data, which includes two steps:\n",
    "\n",
    "1. Calculating temporal mean for the Sentinel-1 granules acquired between the months \\[m1, m2\\]. This is done by **s1_build_vrt.py** and **calc_vrt_stats.py**.\n",
    "\n",
    "2. Removing left/right (cross-track) edge pixels where the border noise is prominent. This is done by **remove_edges.py**.\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **srcpath**\n",
    "  \n",
    "      Source path to where processed granules are stored.\n",
    "      Supported dstpaths: AWS S3 - s3://your_bucket/some_prefix\n",
    "                          GCS - gs://your_bucket/some_prefix\n",
    "                          Local storage - your_local_path\n",
    "  \n",
    "  **year**\n",
    "      \n",
    "      Year of granules to be processed.\n",
    "\n",
    "  \n",
    "### Optional Arguments ###\n",
    "\n",
    "  **--pf path_frame**\n",
    "  \n",
    "      'path_frame' of granules to be processed.\n",
    "      If path_frame not provided, all path_frame's under srcpath/year will be processed.\n",
    "  \n",
    "  **--m1 m1**\n",
    "  \n",
    "      Granules with acquisition month >= m1 will be processed\n",
    "  \n",
    "  **--m2 m2**\n",
    "  \n",
    "      Granules with acquisition month <= m2 will be processed\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* The processing will be slow if srcpath is on AWS S3 or GCS because it requires heavy network I/O between the cloud and your local machine. If srcpath is on AWS S3, it is strongly recommended that you run the processing on AWS EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here\n",
    "# print out vrts that are generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Prepare ALOS-2 tiles ##\n",
    "\n",
    "## Download ALOS/ALOS-2 Mosaic ##\n",
    "\n",
    "### alos2_download_mosaic.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % alos2_download_mosaic.py [-h] aoi year dst\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Download ALOS/ALOS-2 Mosaic data from JAXA website.\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **aoi**\n",
    "\n",
    "      shp/geojson of area of interest (AOI)\n",
    "\n",
    "  **year**\n",
    "\n",
    "      Year\n",
    "\n",
    "  **dst**\n",
    "\n",
    "      Destination location (s3:// or gs:// or local paths). Downloaded data will be stored under dst/year/tarfiles/\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* Downloading ALOS/ALOS-2 Mosaic data requires a JAXA account, which can be registered from: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/registration.htm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALOS-2 Processing ##\n",
    "\n",
    "### alos2_proc.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % alos2_proc.py [-h] proj_dir aoi year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Process ALOS-2 tiles by appyling an Enhanced Lee Filter (why? is that all it does?)\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **proj_dir**\n",
    "\n",
    "      project directory (s3:// or gs:// or local dir). ALOS/ALOS-2 mosaic data (.tar.gz) will be stored under\n",
    "      proj_dir/alos2_mosaic/year/tarfiles/\n",
    "\n",
    "  **aoi**\n",
    "\n",
    "      shp/geojson of area of interest (AOI)\n",
    "\n",
    "  **year**\n",
    "\n",
    "      year\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Prepare Landsat Tiles ##\n",
    "\n",
    "## Export Landsat NDVI ##\n",
    "\n",
    "### gee_export_landsat_ndvi.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % gee_export_landsat_ndvi.py [-h] sitename tiles res year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Submit GEE processing for Landsat NDVI. \n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **sitename**\n",
    "\n",
    "      sitename \n",
    "\n",
    "  **tiles**\n",
    "\n",
    "      shp/geojson file containing tiles onto which output raster will be resampled\n",
    "\n",
    "  **res**\n",
    "\n",
    "      Resolution\n",
    "      \n",
    "  **year**\n",
    "\n",
    "      Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Prepare MODIS Tree Cover Tiles ##\n",
    "\n",
    "## Export MODIS TC ##\n",
    "\n",
    "### gee_export_modis_tc.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % gee_export_modis_tc.py [-h] sitename tiles res year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Submit GEE processing for MODIS tree cover\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **sitename**\n",
    "\n",
    "      sitename \n",
    "\n",
    "  **tiles**\n",
    "\n",
    "      shp/geojson file containing tiles onto which output raster will be resampled\n",
    "\n",
    "  **res**\n",
    "\n",
    "      Resolution\n",
    "      \n",
    "  **year**\n",
    "\n",
    "      Year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Build Stacks ##\n",
    "\n",
    "### build_stacks.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % build_stacks.py [-h] [--sitename sitename] proj_dir tiles year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Build 8 band stacks that include (C-VV / C-VH / C-INC / L-HH / L-HV / L-INC / NDVI / TC)\n",
    "   \n",
    "### Positional Arguments ###\n",
    "\n",
    "  **proj_dir**\n",
    "   \n",
    "      project directory (s3:// or gs:// or local dirs)\n",
    "  \n",
    "  **tiles**\n",
    "  \n",
    "      shp/geojson file that contains tiles for the output stacks\n",
    "      \n",
    "  **year** \n",
    "  \n",
    "      Year\n",
    "      \n",
    "###  Optional Arguements ###\n",
    "      \n",
    "  **sitename**\n",
    "  \n",
    "      sitename. If sitename not specified, proj_dir basename is used at sitename. \n",
    "  \n",
    "### Notes ###\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
