{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stack Preparation (data-prep)\n",
    "\n",
    "## Set data-prep conda environment ##\n",
    "\n",
    "To create data-prep and install required packages:\n",
    "\n",
    "```\n",
    "(base) % conda env create -f data-prep-env.yml\n",
    "(base) % conda activate data-prep\n",
    "```\n",
    "\n",
    "Activate the \"data-prep\" env:\n",
    "```\n",
    "(base) % conda activate data-prep\n",
    "(data-prep) % \n",
    "```\n",
    "\n",
    "## Sentinel-1 ##\n",
    "\n",
    "### Search Sentinel-1 granules on [ASF Vertex](https://search.asf.alaska.edu/#/) ###\n",
    "\n",
    "1. Sign in using your Earthdata credentials. If you haven't used ASF Vertex before, you will need to agree their terms in order to use their HyP3 processing.\n",
    "\n",
    "2. Use following \"Additionl Filters\" when searching granules for your AOI:\n",
    "\n",
    "    * File Type: L1 Detected High-Res Dual-Pol (GRD-HD)\n",
    "    * Beam Mode: IW\n",
    "    * Polarization: VV+VH\n",
    "\n",
    "![vertex_search_filters](img/vertex_search_filters.png)\n",
    "\n",
    "3. Add selected granules into download queue:\n",
    "\n",
    "![vertex_add_queue](img/vertex_add_queue.png)\n",
    "\n",
    "4. Download metadata files. At least download one csv or geojson file, which will be used for submitting HyP3 jobs.\n",
    "\n",
    "![vertex_download_metadata](img/vertex_download_metadata.png)\n",
    "\n",
    "5. Clear the selected granules in the downloads. Do not download these GRD-HD products as we will submit HyP3 jobs to apply radiometric terrain correction (RTC) to them.\n",
    "\n",
    "### Submit HyP3 RTC jobs ###\n",
    "\n",
    "Use **s1_submit_hyp3_jobs.py** to submit HyP3 jobs and optionally copy or download the processed granules to the following destination:\n",
    "\n",
    "* AWS S3 bucket - s3://bucket/prefix\n",
    "\n",
    "* Google Cloud Storage - gs://bucket/prefix\n",
    "\n",
    "* Local storage\n",
    "\n",
    "Since ASF HyP3 stores the processed granules on their AWS S3 buckets, the data transfer will be much faster if you set up your S3 bucket to host these data.\n",
    "\n",
    "Usage of **s1_submit_hyp3_jobs.py**:\n",
    "\n",
    "```\n",
    "(data-prep) python s1_submit_hyp3_jobs.py -h\n",
    "\n",
    "usage: s1_submit_hyp3_jobs.py [-h] [--dst dstpath] csv/geojson\n",
    "\n",
    "submit ASF HyP3 RTC processing jobs for Sentinel-1 granules\n",
    "\n",
    "positional arguments:\n",
    "  csv/geojson    metadata file downloaded from ASF Vertex after data search\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help     show this help message and exit\n",
    "  --dst dstpath  destination path to store processed granules (AWS S3 - s3://dstpath, GCS - gs://dstpath, local storage - dstpath)\n",
    "```\n",
    "\n",
    "To set up AWS CLI credentials (required if your dstpath is S3 or GCS):\n",
    "\n",
    "```\n",
    "(data-prep) % aws configure\n",
    "```\n",
    "\n",
    "where you will be asked for your aws_access_key_id and aws_secret_access_key.\n",
    "\n",
    "To set up Google Cloud gsutil tool (required if your dstpath is GCS):\n",
    "\n",
    "```\n",
    "(data-prep) % gsutil config\n",
    "```\n",
    "\n",
    "Then you will be prompted to sign in using your Google credentials. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
